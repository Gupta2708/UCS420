{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izoZHAE1xEST"
      },
      "source": [
        "Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports,\n",
        "technology, food, books, etc.).\n",
        "1. Convert text to lowercase and remove punctuaƟon.\n",
        "2. Tokenize the text into words and sentences.\n",
        "3. Remove stopwords (using NLTK's stopwords list).\n",
        "4. Display word frequency distribuƟon (excluding stopwords)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9jH_-NtxESY",
        "outputId": "65c0a2dc-10a4-4e8b-d8ed-72ef3284d6fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Text:\n",
            "\n",
            "space exploration has always fascinated humanity pushing the boundaries of our knowledge and imagination \n",
            "innovations in rocket technology have made it possible to send spacecraft beyond our solar system \n",
            "missions to mars like those led by nasa and spacex aim to uncover the secrets of the red planet \n",
            "satellites orbiting earth help us monitor climate change and improve global communication \n",
            "the future of space travel promises new frontiers and endless possibilities for discovery\n",
            "\n",
            "\n",
            "Word Tokens:\n",
            "['space', 'exploration', 'has', 'always', 'fascinated', 'humanity', 'pushing', 'the', 'boundaries', 'of', 'our', 'knowledge', 'and', 'imagination', 'innovations', 'in', 'rocket', 'technology', 'have', 'made', 'it', 'possible', 'to', 'send', 'spacecraft', 'beyond', 'our', 'solar', 'system', 'missions', 'to', 'mars', 'like', 'those', 'led', 'by', 'nasa', 'and', 'spacex', 'aim', 'to', 'uncover', 'the', 'secrets', 'of', 'the', 'red', 'planet', 'satellites', 'orbiting', 'earth', 'help', 'us', 'monitor', 'climate', 'change', 'and', 'improve', 'global', 'communication', 'the', 'future', 'of', 'space', 'travel', 'promises', 'new', 'frontiers', 'and', 'endless', 'possibilities', 'for', 'discovery']\n",
            "\n",
            "Sentence Tokens:\n",
            "['\\nspace exploration has always fascinated humanity pushing the boundaries of our knowledge and imagination \\ninnovations in rocket technology have made it possible to send spacecraft beyond our solar system \\nmissions to mars like those led by nasa and spacex aim to uncover the secrets of the red planet \\nsatellites orbiting earth help us monitor climate change and improve global communication \\nthe future of space travel promises new frontiers and endless possibilities for discovery\\n']\n",
            "\n",
            "Filtered Words (without stopwords):\n",
            "['space', 'exploration', 'always', 'fascinated', 'humanity', 'pushing', 'boundaries', 'knowledge', 'imagination', 'innovations', 'rocket', 'technology', 'made', 'possible', 'send', 'spacecraft', 'beyond', 'solar', 'system', 'missions', 'mars', 'like', 'led', 'nasa', 'spacex', 'aim', 'uncover', 'secrets', 'red', 'planet', 'satellites', 'orbiting', 'earth', 'help', 'us', 'monitor', 'climate', 'change', 'improve', 'global', 'communication', 'future', 'space', 'travel', 'promises', 'new', 'frontiers', 'endless', 'possibilities', 'discovery']\n",
            "\n",
            "Word Frequency Distribution (Excluding Stopwords):\n",
            "space: 2\n",
            "exploration: 1\n",
            "always: 1\n",
            "fascinated: 1\n",
            "humanity: 1\n",
            "pushing: 1\n",
            "boundaries: 1\n",
            "knowledge: 1\n",
            "imagination: 1\n",
            "innovations: 1\n",
            "rocket: 1\n",
            "technology: 1\n",
            "made: 1\n",
            "possible: 1\n",
            "send: 1\n",
            "spacecraft: 1\n",
            "beyond: 1\n",
            "solar: 1\n",
            "system: 1\n",
            "missions: 1\n",
            "mars: 1\n",
            "like: 1\n",
            "led: 1\n",
            "nasa: 1\n",
            "spacex: 1\n",
            "aim: 1\n",
            "uncover: 1\n",
            "secrets: 1\n",
            "red: 1\n",
            "planet: 1\n",
            "satellites: 1\n",
            "orbiting: 1\n",
            "earth: 1\n",
            "help: 1\n",
            "us: 1\n",
            "monitor: 1\n",
            "climate: 1\n",
            "change: 1\n",
            "improve: 1\n",
            "global: 1\n",
            "communication: 1\n",
            "future: 1\n",
            "travel: 1\n",
            "promises: 1\n",
            "new: 1\n",
            "frontiers: 1\n",
            "endless: 1\n",
            "possibilities: 1\n",
            "discovery: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Sample text\n",
        "text = \"\"\"\n",
        "Space exploration has always fascinated humanity, pushing the boundaries of our knowledge and imagination.\n",
        "Innovations in rocket technology have made it possible to send spacecraft beyond our solar system.\n",
        "Missions to Mars, like those led by NASA and SpaceX, aim to uncover the secrets of the Red Planet.\n",
        "Satellites orbiting Earth help us monitor climate change and improve global communication.\n",
        "The future of space travel promises new frontiers and endless possibilities for discovery.\n",
        "\"\"\"\n",
        "\n",
        "# 1. Lowercase\n",
        "text_lower = text.lower()\n",
        "\n",
        "# 2. Remove punctuation\n",
        "text_clean = text_lower.translate(str.maketrans('', '', string.punctuation))\n",
        "print(\"Cleaned Text:\")\n",
        "print(text_clean)\n",
        "\n",
        "# 3. Tokenize\n",
        "words = text_clean.split()\n",
        "sentences = text_clean.split('.')\n",
        "print(\"\\nWord Tokens:\")\n",
        "print(words)\n",
        "\n",
        "print(\"\\nSentence Tokens:\")\n",
        "print(sentences)\n",
        "\n",
        "# 4. Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "print(\"\\nFiltered Words (without stopwords):\")\n",
        "print(filtered_words)\n",
        "\n",
        "# 5. Word Frequency Distribution\n",
        "word_freq = Counter(filtered_words)\n",
        "print(\"\\nWord Frequency Distribution (Excluding Stopwords):\")\n",
        "for word, freq in word_freq.items():\n",
        "    print(f\"{word}: {freq}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOQWnGV4xESc"
      },
      "source": [
        "Q2: Stemming and LemmaƟzaƟon\n",
        "1. Take the tokenized words from QuesƟon 1 (aŌer stopword removal).\n",
        "2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
        "3. Apply lemmaƟzaƟon using NLTK's WordNetLemmaƟzer.\n",
        "4. Compare and display results of both techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g3m2bJmxESd",
        "outputId": "ab9fda6b-84e8-474a-d5d2-feea4028bf72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemming (PorterStemmer): ['space', 'explor', 'alway', 'fascin', 'human', 'push', 'boundari', 'knowledg', 'imagin', 'innov', 'rocket', 'technolog', 'made', 'possibl', 'send', 'spacecraft', 'beyond', 'solar', 'system', 'mission', 'mar', 'like', 'led', 'nasa', 'spacex', 'aim', 'uncov', 'secret', 'red', 'planet', 'satellit', 'orbit', 'earth', 'help', 'us', 'monitor', 'climat', 'chang', 'improv', 'global', 'commun', 'futur', 'space', 'travel', 'promis', 'new', 'frontier', 'endless', 'possibl', 'discoveri']\n",
            "\n",
            "Stemming (LancasterStemmer): ['spac', 'expl', 'alway', 'fascin', 'hum', 'push', 'bound', 'knowledg', 'imagin', 'innov', 'rocket', 'technolog', 'mad', 'poss', 'send', 'spacecraft', 'beyond', 'sol', 'system', 'miss', 'mar', 'lik', 'led', 'nas', 'spacex', 'aim', 'uncov', 'secret', 'red', 'planet', 'satellit', 'orbit', 'ear', 'help', 'us', 'monit', 'clim', 'chang', 'improv', 'glob', 'commun', 'fut', 'spac', 'travel', 'prom', 'new', 'fronty', 'endless', 'poss', 'discovery']\n",
            "\n",
            "Lemmatization: ['space', 'exploration', 'always', 'fascinated', 'humanity', 'pushing', 'boundary', 'knowledge', 'imagination', 'innovation', 'rocket', 'technology', 'made', 'possible', 'send', 'spacecraft', 'beyond', 'solar', 'system', 'mission', 'mar', 'like', 'led', 'nasa', 'spacex', 'aim', 'uncover', 'secret', 'red', 'planet', 'satellite', 'orbiting', 'earth', 'help', 'u', 'monitor', 'climate', 'change', 'improve', 'global', 'communication', 'future', 'space', 'travel', 'promise', 'new', 'frontier', 'endless', 'possibility', 'discovery']\n"
          ]
        }
      ],
      "source": [
        "# Initialize stemmers and lemmatizer\n",
        "porter_stemmer = PorterStemmer()\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply stemming using PorterStemmer and LancasterStemmer\n",
        "porter_stemmed = [porter_stemmer.stem(word) for word in filtered_words]\n",
        "lancaster_stemmed = [lancaster_stemmer.stem(word) for word in filtered_words]\n",
        "\n",
        "# Apply lemmatization using WordNetLemmatizer\n",
        "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "# Compare and display results of both techniques\n",
        "print(\"\\nStemming (PorterStemmer):\", porter_stemmed)\n",
        "print(\"\\nStemming (LancasterStemmer):\", lancaster_stemmed)\n",
        "print(\"\\nLemmatization:\", lemmatized)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QF4rGE7xESe"
      },
      "source": [
        "Q3. Regular Expressions and Text Spliƫng\n",
        "1. Take their original text from QuesƟon 1.\n",
        "2. Use regular expressions to:\n",
        "a. Extract all words with more than 5 leƩers.\n",
        "b. Extract all numbers (if any exist in their text).\n",
        "c. Extract all capitalized words.\n",
        "3. Use text spliƫng techniques to:\n",
        "a. Split the text into words containing only alphabets (removing digits and special\n",
        "characters).\n",
        "b. Extract words starƟng with a vowel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfyAJzCwxESf",
        "outputId": "5ebeb880-abc1-4708-d46a-405259d90744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Words with more than 5 letters: ['exploration', 'always', 'fascinated', 'humanity', 'pushing', 'boundaries', 'knowledge', 'imagination', 'innovations', 'rocket', 'technology', 'possible', 'spacecraft', 'beyond', 'system', 'missions', 'spacex', 'uncover', 'secrets', 'planet', 'satellites', 'orbiting', 'monitor', 'climate', 'change', 'improve', 'global', 'communication', 'future', 'travel', 'promises', 'frontiers', 'endless', 'possibilities', 'discovery']\n",
            "\n",
            "Numbers found in text: []\n",
            "\n",
            "Capitalized words: []\n",
            "\n",
            "Alphabetic words: ['space', 'exploration', 'has', 'always', 'fascinated', 'humanity', 'pushing', 'the', 'boundaries', 'of', 'our', 'knowledge', 'and', 'imagination', 'innovations', 'in', 'rocket', 'technology', 'have', 'made', 'it', 'possible', 'to', 'send', 'spacecraft', 'beyond', 'our', 'solar', 'system', 'missions', 'to', 'mars', 'like', 'those', 'led', 'by', 'nasa', 'and', 'spacex', 'aim', 'to', 'uncover', 'the', 'secrets', 'of', 'the', 'red', 'planet', 'satellites', 'orbiting', 'earth', 'help', 'us', 'monitor', 'climate', 'change', 'and', 'improve', 'global', 'communication', 'the', 'future', 'of', 'space', 'travel', 'promises', 'new', 'frontiers', 'and', 'endless', 'possibilities', 'for', 'discovery']\n",
            "\n",
            "Words starting with a vowel: ['exploration', 'always', 'of', 'our', 'and', 'imagination', 'innovations', 'in', 'it', 'our', 'and', 'aim', 'uncover', 'of', 'orbiting', 'earth', 'us', 'and', 'improve', 'of', 'and', 'endless']\n"
          ]
        }
      ],
      "source": [
        "# 1. Use regular expressions to extract:\n",
        "# a. All words with more than 5 letters\n",
        "long_words = re.findall(r'\\b\\w{6,}\\b', text_clean)\n",
        "print(\"\\nWords with more than 5 letters:\", long_words)\n",
        "\n",
        "# b. All numbers\n",
        "numbers = re.findall(r'\\b\\d+\\b', text_clean)\n",
        "print(\"\\nNumbers found in text:\", numbers)\n",
        "\n",
        "# c. All capitalized words\n",
        "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', text_clean)\n",
        "print(\"\\nCapitalized words:\", capitalized_words)\n",
        "\n",
        "# 2. Use text splitting techniques:\n",
        "# a. Split the text into words containing only alphabets\n",
        "alphabetic_words = re.findall(r'\\b[a-zA-Z]+\\b', text_clean)\n",
        "print(\"\\nAlphabetic words:\", alphabetic_words)\n",
        "\n",
        "# b. Extract words starting with a vowel\n",
        "vowel_words = re.findall(r'\\b[aeiouAEIOU]\\w*\\b', text_clean)\n",
        "print(\"\\nWords starting with a vowel:\", vowel_words)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKRiHKlVxESh"
      },
      "source": [
        "Q4. Custom TokenizaƟon & Regex-based Text Cleaning\n",
        "1. Take original text from QuesƟon 1.\n",
        "2. Write a custom tokenizaƟon funcƟon that:\n",
        "a. Removes punctuaƟon and special symbols, but keeps contracƟons (e.g.,\n",
        "\"isn't\" should not be split into \"is\" and \"n't\").\n",
        "b. Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains\n",
        "a single token).\n",
        "c. Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\"\n",
        "should remain as is).\n",
        "3. Use Regex SubsƟtuƟons (re.sub) to:\n",
        "a. Replace email addresses with '<EMAIL>' placeholder.\n",
        "b. Replace URLs with '<URL>' placeholder.\n",
        "c. Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with\n",
        "'<PHONE>' placeholder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvSCqS8rxESh",
        "outputId": "8dd53d4c-fabd-426e-c4ab-3faa2b67b2f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Custom Tokenized Text: ['artificial', 'intelligence', 'is', 'transforming', 'various', 'industries', 'making', 'tasks', 'easier', 'and', 'more', 'efficient', 'from', 'healthcare', 'to', 'finance', 'ai-powered', 'systems', 'are', 'becoming', 'indispensable', 'tools', 'for', 'decision-making', 'technologies', 'like', 'deep', 'learning', 'and', 'machine', 'learning', 'allow', 'for', 'better', 'predictions', 'automated', 'processes', 'and', 'enhanced', 'data', 'analysis', 'email', 'me', 'at', 'exampleemailcom', 'or', 'visit', 'httpexamplecom', 'for', 'more', 'information', 'call', 'us', 'at', '91', '9876543210']\n",
            "\n",
            "Text with Emails, URLs, and Phone Numbers Replaced:\n",
            "\n",
            "Artificial Intelligence is transforming various industries, making tasks easier and more efficient.\n",
            "From healthcare to finance, AI-powered systems are becoming indispensable tools for decision-making.\n",
            "Technologies like deep learning and machine learning allow for better predictions, automated processes, and enhanced data analysis.\n",
            "Email me at <EMAIL> or visit <URL> for more information. Call us at <PHONE>.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Sample text (original text from Q1)\n",
        "text = \"\"\"\n",
        "Artificial Intelligence is transforming various industries, making tasks easier and more efficient.\n",
        "From healthcare to finance, AI-powered systems are becoming indispensable tools for decision-making.\n",
        "Technologies like deep learning and machine learning allow for better predictions, automated processes, and enhanced data analysis.\n",
        "Email me at example@email.com or visit http://example.com for more information. Call us at +91 9876543210.\n",
        "\"\"\"\n",
        "\n",
        "# 1. Custom Tokenization function\n",
        "def custom_tokenizer(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation except for contractions and hyphenated words\n",
        "    text = re.sub(r\"[^\\w\\s'-]\", '', text)\n",
        "\n",
        "    # Tokenize words, considering spaces as the separator\n",
        "    tokens = re.findall(r\"[\\w'-]+(?:\\.\\d+)?\", text)  # Include decimal numbers as part of the token\n",
        "    return tokens\n",
        "\n",
        "custom_tokens = custom_tokenizer(text)\n",
        "print(\"\\nCustom Tokenized Text:\", custom_tokens)\n",
        "\n",
        "# 2. Regex substitutions for cleaning\n",
        "# a. Replace email addresses with '<EMAIL>'\n",
        "text_with_emails = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', '<EMAIL>', text)\n",
        "\n",
        "# b. Replace URLs with '<URL>'\n",
        "text_with_urls = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '<URL>', text_with_emails)\n",
        "\n",
        "# c. Replace phone numbers with '<PHONE>'\n",
        "text_cleaned = re.sub(r'(\\+?\\d{1,2}\\s?)?(\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4})', '<PHONE>', text_with_urls)\n",
        "\n",
        "print(\"\\nText with Emails, URLs, and Phone Numbers Replaced:\")\n",
        "print(text_cleaned)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}